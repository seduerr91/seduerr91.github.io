---
title: Yelp ML SD Case
tags: [Coding]
style: fill
color: secondary
description: A comprehensive Machine Learning Systems Design discussion around building a recommendation model for a Yelp-like platform (local business recommendation) using a two-tower (a.k.a. two-tower embedding) approach.
---


Below is a comprehensive Machine Learning Systems Design discussion around building a recommendation model for a Yelp-like platform (local business recommendation) using a two-tower (a.k.a. two-tower embedding) approach. We will then walk through a notional end-to-end Jupyter Notebook that implements this idea with PyTorch. Assume this content is for an interview setting (e.g., Facebook/Meta) where you are asked to demonstrate your skills with recommending relevant items to users based on a user and item tower architecture.

────────────────────────────────────────────────────────────────
1. PROBLEM DEFINITION AND HIGH-LEVEL DESIGN
────────────────────────────────────────────────────────────────

1.1 Use Case: Yelp Feed Ranking
--------------------------------
• We have a user base (consumers) who browse local businesses (restaurants, coffee shops, bars, etc.).  
• We want to rank items (restaurants) in a user’s feed by predicting the “score” or relevance for each user‐item pair. The system should push the best matches to the top of the user’s feed.  

1.2 Two-Tower Architecture Overview
-----------------------------------  
A two-tower model (sometimes referred to as a dual-encoder) is often used in recommendation systems to separately learn embeddings for:  
• Users (user tower)  
• Items (item/business tower)  

Each tower produces an embedding of the respective entity. The final score is computed as a dot product (or another similarity function) between user and item embeddings.

1.3 Advantages of Two-Tower Models
-----------------------------------
• Scalability: You can precompute embeddings for users and items to speed up retrieval.  
• Flexibility: The towers can handle different input features (e.g., user profile, user history vs. item metadata, item textual descriptions, etc.).  
• Online or Near Real-Time Serving: You can retrieve top items for a user by computing the dot product with user embedding and all candidate item embeddings.  

1.4 Embedding Flow
-------------------
1. User Tower: Extract the user’s features (e.g., user ID, user’s location, user’s average rating pattern, or user’s textual embeddings from reviews, etc.). Feed these into a neural network tower, producing a user embedding.  
2. Item Tower: Extract item/business features (business ID, location, business category, textual embeddings from business descriptions, etc.). Feed them into another neural network tower, producing an item embedding.  
3. Prediction Score: The dot product (or similarity) of the user embedding and the item embedding is the predicted “score” used to rank items.

1.5 Data & Features
--------------------
In a more realistic setting, we would have a large dataset of user actions:  
• User IDs  
• Item/business IDs  
• Interactions (click, view, rating, like, etc.)  
• Optional contextual features (time of day, user’s location, etc.)  

For demonstration, we’ll craft a small synthetic dataset.

1.6 Training Objective
-----------------------
• Typically, we train a two-tower model for implicit feedback, using a binary classification or ranking loss (e.g., cross entropy or pairwise hinge loss).  
• In this example, we can do a simple rating-based or “implicit feedback” style approach, using (user, item, label). The label might be 1 if user engaged or 0 otherwise.  
• Alternatively, one might use pairwise ranking losses like Bayesian Personalized Ranking (BPR).

1.7 Metrics
------------
Common recommendation metrics include:  
• Mean Average Precision @ K (MAP@K)  
• Normalized Discounted Cumulative Gain @ K (NDCG@K)  
• Recall @ K, Precision @ K  
• For simpler binary classification tasks, we sometimes use AUC or log-loss.  

────────────────────────────────────────────────────────────────
2. SAMPLE DATA OVERVIEW
────────────────────────────────────────────────────────────────
Our mock dataset will be extremely small for demonstration:
• user_id: an integer ID for each user  
• business_id: an integer ID for each restaurant  
• label: whether the user engaged (1) or did not engage (0) with the listing  

We will generate features:
• user tower input: user ID, user’s city, user preference vector  
• item tower input: business ID, category, business location vector  

────────────────────────────────────────────────────────────────
3. MODEL TRAINING FLOW
────────────────────────────────────────────────────────────────

3.1 Input Pipeline
-------------------
1) Load data (train, valid/test split).  
2) Map categorical features to embeddings or one-hot vectors.  
3) Normalize numeric features as needed.  

3.2 Two-Tower Model Implementation
-----------------------------------
• Construct user tower: user embedding plus optional layers.  
• Construct item tower: item embedding plus optional layers.  
• Multiply user embedding and item embedding to get a logit.  
• Apply sigmoid or cross-entropy for training.

3.3 Loss Definition
--------------------
Simple cross-entropy:
loss = - (1/N) * Σ [y_i * log(ŷ_i) + (1 - y_i) * log(1 - ŷ_i)],  
where y_i is the true label (0 or 1), and ŷ_i is the predicted probability that the user interacts with the item.

3.4 Training Loop
------------------
For each mini-batch:  
1) Forward pass: user tower → user_emb, item tower → item_emb.  
2) Score = dot(user_emb, item_emb).  
3) Predictions = sigmoid(score).  
4) Compute cross-entropy loss.  
5) Backprop + optimizer step.  

3.5 Serving / Inference
------------------------
• Precompute item embeddings for all items.  
• For a given user, compute user embedding from the user tower.  
• Dot product user embedding with all item embeddings to get scores → retrieve top K.  

────────────────────────────────────────────────────────────────
4. DETAILED JUPYTER NOTEBOOK EXAMPLE (PyTorch)
────────────────────────────────────────────────────────────────

Below is a notional notebook walk-through. In a real interview, you might not be able to fully run all cells, but you should be able to conceptually walk through each step.

--------------------------------------------------------------------------------
NOTEBOOK START
--------------------------------------------------------------------------------

-------------------------------------------------------------------------------
Markdown Cell #1 - Notebook Title & Intro
-------------------------------------------------------------------------------
# Two-Tower Recommendation Model in PyTorch
In this notebook, we demonstrate a two-tower recommendation system using synthetic data for a Yelp-like feed ranking scenario.

-------------------------------------------------------------------------------
Code Cell #1 - Imports and Basic Setup
-------------------------------------------------------------------------------
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import random

# For reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

-------------------------------------------------------------------------------
Markdown Cell #2 - Generate Synthetic Data
-------------------------------------------------------------------------------
# Let's create a small synthetic dataset that simulates the following columns:
#   user_id, business_id, user_city_id, business_category_id, label
# For demonstration, we will keep it small, but well-labeled.

-------------------------------------------------------------------------------
Code Cell #2 - Data Generation
-------------------------------------------------------------------------------
num_users = 10
num_businesses = 20

# Generate random features
user_ids = np.random.choice(np.arange(num_users), size=1000, replace=True)
business_ids = np.random.choice(np.arange(num_businesses), size=1000, replace=True)

# Some random city IDs for users and category IDs for businesses
user_city_ids = np.random.choice([0, 1, 2], size=1000, replace=True)
business_cat_ids = np.random.choice([0, 1, 2, 3], size=1000, replace=True)

# Label: Let's say label=1 if user_id and business_id match certain random pattern
def generate_label(u, b):
    # Random rule: user and business are more likely matched if user_id % 3 == business_id % 3
    return 1 if (u % 3 == b % 3) else 0

labels = np.array([generate_label(u, b) for u, b in zip(user_ids, business_ids)])

df = pd.DataFrame({
    'user_id': user_ids,
    'business_id': business_ids,
    'user_city_id': user_city_ids,
    'business_cat_id': business_cat_ids,
    'label': labels
})

print(df.head(10))

-------------------------------------------------------------------------------
Markdown Cell #3 - Dataset and DataLoader
-------------------------------------------------------------------------------
# We now define a PyTorch Dataset to handle the user, item, and label tuples.

-------------------------------------------------------------------------------
Code Cell #3 - Define Dataset Class
-------------------------------------------------------------------------------
class YelpTwoTowerDataset(Dataset):
    def __init__(self, dataframe):
        self.data = dataframe
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        user_id = row['user_id']
        business_id = row['business_id']
        user_city_id = row['user_city_id']
        business_cat_id = row['business_cat_id']
        label = row['label']
        return {
            'user_id': torch.tensor(user_id, dtype=torch.long),
            'business_id': torch.tensor(business_id, dtype=torch.long),
            'user_city_id': torch.tensor(user_city_id, dtype=torch.long),
            'business_cat_id': torch.tensor(business_cat_id, dtype=torch.long),
            'label': torch.tensor(label, dtype=torch.float)
        }

# Create train/test split
train_size = int(0.8 * len(df))
test_size = len(df) - train_size

train_df = df.iloc[:train_size].reset_index(drop=True)
test_df = df.iloc[train_size:].reset_index(drop=True)

train_dataset = YelpTwoTowerDataset(train_df)
test_dataset = YelpTwoTowerDataset(test_df)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

-------------------------------------------------------------------------------
Markdown Cell #4 - Building the Two-Tower Model
-------------------------------------------------------------------------------
# We'll create two separate towers (using embeddings) and then produce a final score by dot product.

-------------------------------------------------------------------------------
Code Cell #4 - Define Model Architecture
-------------------------------------------------------------------------------
class TwoTowerRecModel(nn.Module):
    def __init__(self, 
                 num_users, 
                 num_items, 
                 num_user_city, 
                 num_business_cat, 
                 embedding_dim=8):
        super(TwoTowerRecModel, self).__init__()
        
        # For the user tower
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.user_city_embedding = nn.Embedding(num_user_city, embedding_dim)
        
        # For the business tower
        self.business_embedding = nn.Embedding(num_items, embedding_dim)
        self.business_cat_embedding = nn.Embedding(num_business_cat, embedding_dim)
        
        # Additional linear layers can be added if desired
        # We'll just keep it simple: sum the embeddings for now
        
    def forward(self, user_id, user_city_id, business_id, business_cat_id):
        # user tower
        user_emb = self.user_embedding(user_id)
        user_city_emb = self.user_city_embedding(user_city_id)
        user_tower_emb = user_emb + user_city_emb  # shape [batch_size, embedding_dim]
        
        # business tower
        business_emb = self.business_embedding(business_id)
        business_cat_emb = self.business_cat_embedding(business_cat_id)
        business_tower_emb = business_emb + business_cat_emb  # shape [batch_size, embedding_dim]
        
        # Dot product to get the score - shape [batch_size]
        scores = (user_tower_emb * business_tower_emb).sum(dim=1)
        
        return scores  # raw score (logit)
    
# Instantiate the model
num_user_city = df['user_city_id'].nunique()
num_business_cat = df['business_cat_id'].nunique()

model = TwoTowerRecModel(num_users=num_users,
                         num_items=num_businesses,
                         num_user_city=num_user_city,
                         num_business_cat=num_business_cat,
                         embedding_dim=8).to(device)

-------------------------------------------------------------------------------
Markdown Cell #5 - Defining Loss, Optimizer, and Training
-------------------------------------------------------------------------------
# We'll use binary cross-entropy with logits as the loss function.
# The optimizer will be Adam. Our metrics will include average loss and AUC (here we might approximate or omit for brevity).

-------------------------------------------------------------------------------
Code Cell #5 - Set Criterion and Optimizer
-------------------------------------------------------------------------------
# Binary Cross Entropy with Logits
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

-------------------------------------------------------------------------------
Markdown Cell #6 - Training Loop
-------------------------------------------------------------------------------
# We'll run a few epochs over our synthetic data. We'll compute the train loss for each epoch.

-------------------------------------------------------------------------------
Code Cell #6 - Training
-------------------------------------------------------------------------------
epochs = 5

for epoch in range(epochs):
    model.train()
    total_loss = 0.0
    
    for batch in train_loader:
        user_id = batch['user_id'].to(device)
        business_id = batch['business_id'].to(device)
        user_city_id = batch['user_city_id'].to(device)
        business_cat_id = batch['business_cat_id'].to(device)
        labels = batch['label'].to(device)
        
        optimizer.zero_grad()
        scores = model(user_id, user_city_id, business_id, business_cat_id)
        loss = criterion(scores, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}")

-------------------------------------------------------------------------------
Markdown Cell #7 - Evaluation (Example: Test Loss & AUC)
-------------------------------------------------------------------------------
# Let's evaluate the model on the test set. We'll compute the loss and approximate AUC.

-------------------------------------------------------------------------------
Code Cell #7 - Testing & AUC Calculation
-------------------------------------------------------------------------------
from sklearn.metrics import roc_auc_score

model.eval()
y_true_all = []
y_score_all = []

with torch.no_grad():
    total_test_loss = 0.0
    
    for batch in test_loader:
        user_id = batch['user_id'].to(device)
        business_id = batch['business_id'].to(device)
        user_city_id = batch['user_city_id'].to(device)
        business_cat_id = batch['business_cat_id'].to(device)
        labels = batch['label'].to(device)
        
        scores = model(user_id, user_city_id, business_id, business_cat_id)
        loss = criterion(scores, labels)
        total_test_loss += loss.item()
        
        probs = torch.sigmoid(scores)
        y_true_all.extend(labels.cpu().numpy().tolist())
        y_score_all.extend(probs.cpu().numpy().tolist())
        
    avg_test_loss = total_test_loss / len(test_loader)
    auc = roc_auc_score(y_true_all, y_score_all)

print(f"Test Loss: {avg_test_loss:.4f}, AUC: {auc:.4f}")

-------------------------------------------------------------------------------
Markdown Cell #8 - Example Usage: Retrieving Top-K Recommendations
-------------------------------------------------------------------------------
# For serving, we can retrieve the top-K recommended businesses for a single user by:
#  1) Compute the user tower embedding
#  2) Compute scores against all business embeddings
#  3) Sort in descending order
# We'll demonstrate for a single user.

-------------------------------------------------------------------------------
Code Cell #8 - Top-K Retrieval
-------------------------------------------------------------------------------
def get_user_embedding(model, user_id, user_city_id):
    with torch.no_grad():
        user_id_t = torch.tensor([user_id], dtype=torch.long, device=device)
        user_city_id_t = torch.tensor([user_city_id], dtype=torch.long, device=device)
        user_emb = model.user_embedding(user_id_t) + model.user_city_embedding(user_city_id_t)
        return user_emb.squeeze(0)  # shape [embedding_dim]

def get_business_embedding(model, business_id, business_cat_id):
    with torch.no_grad():
        business_id_t = torch.tensor([business_id], dtype=torch.long, device=device)
        business_cat_id_t = torch.tensor([business_cat_id], dtype=torch.long, device=device)
        business_emb = model.business_embedding(business_id_t) + model.business_cat_embedding(business_cat_id_t)
        return business_emb.squeeze(0)

# Precompute all business embeddings
business_cat_map = df.groupby('business_id')['business_cat_id'].first().to_dict()
business_embs = {}
model.eval()
for b_id in range(num_businesses):
    cat_id = business_cat_map.get(b_id, 0)
    business_embs[b_id] = get_business_embedding(model, b_id, cat_id)

def recommend_for_user(model, user_id, user_city_id, top_k=5):
    user_emb = get_user_embedding(model, user_id, user_city_id)
    scores = []
    for b_id, b_emb in business_embs.items():
        score = torch.dot(user_emb, b_emb).item()
        scores.append((b_id, score))
    # Sort by descending score
    scores.sort(key=lambda x: x[1], reverse=True)
    return scores[:top_k]

# Example usage
sample_user_id = 1
sample_user_city_id = 2  # arbitrary
top_k_businesses = recommend_for_user(model, sample_user_id, sample_user_city_id, top_k=5)
print("User ID:", sample_user_id, "Top-5 Recommendations:", top_k_businesses)

--------------------------------------------------------------------------------
NOTEBOOK END
--------------------------------------------------------------------------------

────────────────────────────────────────────────────────────────
5. ADVANCED CONSIDERATIONS (INTERVIEW-LEVEL DISCUSSION)
────────────────────────────────────────────────────────────────

5.1 Feature Engineering
-----------------------
• Incorporate user’s rating distributions, textual embeddings from user reviews, or frequency of check-ins.  
• Business side: categories, location, average rating, textual embeddings from reviews or descriptions.

5.2 Hard Negative Sampling
--------------------------
• In typical implicit feedback data, a user not interacting with an item does not necessarily mean they dislike it.  
• We often carefully select negative samples from items that the user was exposed to but did not interact with.

5.3 Performance and Scalability
-------------------------------
• Precompute embeddings offline for large-scale retrieval.  
• Use approximate nearest neighbor search libraries (e.g., FAISS) to efficiently retrieve candidate top-K from millions of items.  
• Then do a second-stage re-ranking with a more complex model if needed.

5.4 Online Deployment
---------------------
• Typically, user embeddings can be updated in real-time if user features arrive quickly.  
• Business embeddings can be updated daily or weekly if changes are slower.  

5.5 A/B Testing
---------------
• Once you deploy, measure click-through rate (CTR), dwell time, or other success metrics in an A/B test.  

5.6 Multi-Task Learning
-----------------------
• You may want to optimize multiple objectives: clicks, purchases, likes, etc.  
• A multi-tower approach can be extended to multi-task settings.

────────────────────────────────────────────────────────────────
6. CONCLUSION
────────────────────────────────────────────────────────────────
In a 35-minute interview discussion, you would:
• Describe how the two-tower architecture suits the recommendation task.  
• Walk through the dataset, features, and how embeddings are used.  
• Discuss the training loop with cross-entropy or pairwise loss.  
• Highlight your chosen metrics (AUC, precision@K, NDCG@K), and mention offline vs. online.  
• Emphasize real-world considerations like negative sampling, large-scale retrieval, and model updates.  

The above notebook sketch offers a concise (though synthetic) example of how you might implement a standard two-tower model in PyTorch for a Yelp-like local business recommendation.