<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Using a lot of mails.">
  <meta property="og:description" content="Using a lot of mails.">
  <meta property="og:image" content="https://i.imgur.com/r0yP47Um.png">

  <title>Seb Duerr</title>
  <meta name="description" content="Using a lot of mails.">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

    <a class="navbar-brand" href="/"><h5><b>Seb Duerr</b></h5></a>
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
      <i class="fas fa-1x fa-bars text-themed"></i>
    </button>
  
    <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
      <div class="navbar-nav ml-auto"><a class="nav-item nav-link " href="/datenschutz.html"></a>
  
        <a class="nav-item nav-link " href="/pages/public">Portfolio</a>
  
        <a class="nav-item nav-link " href="/projects/">Projects</a>
  
        <a class="nav-item nav-link active" href="/blog/">Blog</a>
  
        <a class="nav-item nav-link " href="/about/">About</a>
  
        
  
        <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
      </div>
    </div>
  
  </nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1><b>Using a lot of mails.</b></h1>

<p class="post-metadata text-muted">
  10 August 2021 -  
  <b>5 mins read time</b>

  <br>Tags: 
    
    <a class="text-decoration-none no-underline" href="/blog/tags#persuaide">
      <span class="tag badge badge-pill text-primary border border-primary">Persuaide</span>
    </a>
    
    <a class="text-decoration-none no-underline" href="/blog/tags#engineering">
      <span class="tag badge badge-pill text-primary border border-primary">engineering</span>
    </a>
    </p>

<h1 id="Ô∏è-sourcing-large-scale-email-data">‚úâÔ∏è Sourcing Large Scale Email Data</h1>

<p>In order to write intro &amp; extroverted mail texts, we need mail data as a base. The problem about mails is that they are usually highly personal  or proprietary which is why there is little available. While you can crawl a forum or a chat to get data it is hard to collect mail data. 
There is some publicly available mail datasets since the leakage of the Enron email dataset upon its scandal in 2001 or the leakage of Hillary Clinton‚Äôs personal emails in the context of the 2016 US presidential elections, but there quality, timeliness and quantity is rather little. 
Yet, the German team behind webis.de compiled a large scale dataset consisting of 153M emails that got scraped from the GMANE mailing lists. 
While there is a lot of mails relating to computer, os or linux issues, a few million (24.3M) are in more non-technological domains like history, arts, music or business. A statistical overview can be found on the right.
The entirety of the data comes in a nested json structure, that needs parsing and pre-processing to be useful. 
This step is always needed, and there are different packages that can be used to work with. 
Out of preference, the author uses Python Pandas on a Google Colab Infrastructure in the following.</p>

<p><img src="https://i.postimg.cc/tCWCn07r/Screenshot-2021-09-12-at-22-01-21.png" alt="Screenshot-2021-09-12-at-22-01-21.png" />](https://postimg.cc/KRcyCHPL)</p>

<h1 id="an-exemplary-email-">An exemplary email üìÉ</h1>

<p>Before exploring the structure of the entire dataset, we look on a single mail to get an overview. 
The raw email text that is of  interest to us is composed of the: salutation, paragraph and closing (see red rectangular).
Other visible aspects are the quotation of the previous text and the message signature. 
The quotation (quot) may prove useful to evaluate the relation between the two communicators and to understand the perception of the quoted message (think of analysing the degree of positiveness, politeness or acceptance from the responders email). 
Visual markers such as the separators (sep) or the tech may also not be of interest for creating an initial email dataset but could be revisited later.
We do not see much value in evaluating the message signature (msig) since it is usually pre-compiled and not written by a human.</p>

<p><img src="https://i.postimg.cc/WzMLzvSg/Screenshot-2021-09-12-at-22-03-00.png" alt="Screenshot-2021-09-12-at-22-03-00.png" />](https://postimg.cc/2L8X9gsS)</p>

<h1 id="getting-a-data-overview">Getting a Data Overview</h1>

<p>Before writing the first line of code, we take a brief look at all the different columns names (i.e. headers) available and get a bird‚Äôs eye view of the data we are dealing with.
(1) Starting from the highest nesting level ‚ÄòAvailable Columns‚Äô allows us to filter the language we are interested in (en) and allows us to get the groups (i.e., domains in which text is written) that we are actually interested in (e.g., arts, history, business, money). This step drastically reduces the amount of relevant mails to process (since we exclude major topics such as os). Obviously, we want to consider the text_plain as the core of our dataset.
(2) Next, we choose our mail headers, such as subject, from, to, and most importantly the boolean value ‚Äòin_reply_to‚Äò to identify the perception of a mail (see previous slide).
(3) Last, we select all the segments that are of importance to us, such as paragraph, closing, salutation, to get the plain text. Further segments such as quotation may also be of interest. More on that in the excursus: ‚Äòexport mail-response pairs‚Äô.</p>

<p><img src="https://i.postimg.cc/T3NSR1GB/Screenshot-2021-09-12-at-22-04-01.png" alt="Screenshot-2021-09-12-at-22-04-01.png" />](https://postimg.cc/68Rzc98h)</p>

<h1 id="parsing-json">Parsing .json</h1>

<p>At this level the coding section starts, and albeit the introduction part is left out (importing packages, connecting to Google Drive), and reading the json files in, and concatenating them to a pandas dataframe.
Yet, we only have the nested structure of the json files, so they need to be flattened.</p>

<p>Tricky! It is important that we only get the actual email bodies from salutation to ending (e.g., salutation, paragraph, table, paragraph, ending), since they only contain human written sentences. 
The trick is to use the string start and end indicators from the segments and to apply them to the plain text. The illustration in the lower left sketches the idea behind it. 
The parsing happened up to a nesting level of 6, since that was considered to be simple enough mails (and also allowed not to run out of the 32 GB RAM of Google Colab). Batching would have helped to prevent that but it resulted in enough data.</p>

<p><img src="https://i.postimg.cc/DZn1HQHh/Screenshot-2021-09-12-at-22-04-35.png" alt="Screenshot-2021-09-12-at-22-04-35.png" />](https://postimg.cc/p9c9njPc)</p>

<h1 id="-sentence-level-email-data">üíø Sentence-level Email Data</h1>

<p>The objective is to get properly written email styled sentences that are below 512 tokens length. These shall be linguistically and semantically correct under the consideration of the following aspects:</p>

<ul>
  <li>The casing shall be correct.</li>
  <li>Correct verb inflections and number of persons with adjectives, nouns, adverbs, etc.</li>
  <li>Punctuations shall be recognized.</li>
  <li>All code, and non alphanumeric sentences are removed (extensive use of #### or %$#%@&amp;*# shall not happen.</li>
  <li>Duplicates should be removed.</li>
  <li>Extensively long, and very short three word sentences have to be excluded.</li>
  <li>All text fragments are on sentence levels.</li>
</ul>

<p>Therefore, replacements, regex cleanups, and the splitting into sentences based on the statistical model of SpaCy is conducted. Since we are dealing with millions of sentences, it helps to parallelize and batch this process.</p>

<h1 id="-some-data-augmentation-secrets">üí• Some Data Augmentation Secrets</h1>

<p>Lastly, we are going to add some natural language processing sugar to our already almost picture perfect email dataset.
Up until now, we have not accounted for spelling errors, and semantic and syntactic correctness in our dataset.
Luckily, there is ways to outsource and double check these. While I was using Jamspell earlier to correct for spelling mistakes, I encountered GingerIT which offers an API that deals with erroneous writing inconsistencies such as:</p>

<ul>
  <li>Casing</li>
  <li>Verb inflections</li>
  <li>Tense</li>
  <li>Pluralization</li>
  <li>Spelling</li>
</ul>

<p>However, their API is slow (processing a sentence takes roughly one second each and we are processing 1.1M). Therefore, it is helpful to parallelize the process by splitting the dataframe into multiple ones, and parallelizing the data processing. With a factor of 16, I achieved the transformation of the entire dataset in under 24 hours.</p>

<p>Besides, running all sentences through the a distillbert deep neural net that is fine-tuned on the CoLA dataset allows us to get a rating in between 0 and 1, relating to the semantic and syntactic correctness of each sentence.</p>



<div class="pt-5">
  
</div>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Seb Duerr</strong>
  </small>
  <br> <a href="https://seduerr91.github.io/datenschutz">Disclaimer</a>
  
  <div class="container-fluid justify-content-center">
  
  </div>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>