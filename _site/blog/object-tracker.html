<!DOCTYPE html>

<!--
  portfolYOU Jekyll theme by Youssef Raafat
  Free for personal and commercial use under the MIT license
  https://github.com/YoussefRaafatNasry/portfolYOU
-->

<html lang="en" class="h-100">

<head>

  
  
  

  

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:type" content="website">
  <meta property="og:title" content="Tracking Object Movements with Machine Learning (Open CV)">
  <meta property="og:description" content="Plants respond to happy emotions. This article illustrates how to use Machine Learning to show this repeatedly.">
  <meta property="og:image" content="https://i.imgur.com/r0yP47Um.png">

  <title>Seb Duerr</title>
  <meta name="description" content="Plants respond to happy emotions. This article illustrates how to use Machine Learning to show this repeatedly.">
  <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon.ico">

  <!-- Theme style -->
  <script src="/assets/js/theme.js"></script>

  <!-- Font Awesome CDN -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">

  <!-- Bootstrap CSS CDN -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">

  <!-- Animate CSS CDN -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.css">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/assets/css/style.css">

</head>


<body class="h-100 d-flex flex-column">

  <main class="flex-shrink-0 container mt-5">
    <nav class="navbar navbar-expand-lg navbar-themed">

    <a class="navbar-brand" href="/"><h5><b>Seb Duerr</b></h5></a>
  
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
      <i class="fas fa-1x fa-bars text-themed"></i>
    </button>
  
    <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
      <div class="navbar-nav ml-auto">
<a class="nav-item nav-link " href="/datenschutz.html"></a>
  
        <a class="nav-item nav-link " href="/pages/public">Portfolio</a>
  
        <a class="nav-item nav-link " href="/projects/">Projects</a>
  
        <a class="nav-item nav-link active" href="/blog/">Blog</a>
  
        <a class="nav-item nav-link " href="/about/">About</a>
  
        
  
        <span id="theme-toggler" class="nav-item nav-link" role="button" onclick="toggleTheme()"></span>
      </div>
    </div>
  
  </nav>
    <div class="col-lg-10 mx-auto mt-5 markdown-body">
  <h1><b>Tracking Object Movements with Machine Learning (Open CV)</b></h1>

<p class="post-metadata text-muted">
  12 June 2020 -  
  <b>8 mins read time</b>

  <br>Tags: 
    
    <a class="text-decoration-none no-underline" href="/blog/tags#coding">
      <span class="tag badge badge-pill text-primary border border-primary">Coding</span>
    </a>
    </p>

<p><strong>Note: This is very nascent research and until now, not externally validated.</strong></p>

<p>‘Plants respond to positive and negative emotional voices differently.’ - Wait! What? Plants <em>respond</em>? What do they do? The later would be my reaction when I got confronted with the research that I am presenting hereafter.</p>

<h2 id="exemplary-object-tracking-of-a-flying-paprika">Exemplary object tracking of a flying Paprika</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Za6oUCQ0QVU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h2 id="initial-results-from-manual-object-detection">Initial Results from Manual Object Detection</h2>

<p>Initial research from my colleague Josephine indicates that specific plants (we are conducting experiments with the mimosa, <em>i.e. Mimosa Pudica</em>, and the codario, <em>i.e. Codariocalyx Motorius</em>) raise and accelerate their leaves faster if they are exposed to different types of music (1), or vocal emotions (2):</p>

<p>(1) Codario and Different Music Types</p>

<p><img src="https://i.imgur.com/9cwEpuj.png" alt="Music Types"></p>

<p>These graphs illustrate the time the codario takes to respond to different auditive stimuli such as ‘silence’, ‘metal’, ‘techno’, ‘classical’, and ‘yodeling’ (the latter deserves credits to our <a href="https://cci.mit.edu/pgloor/">mentor</a> who is Swiss). One may notice that yodeling was considerably faster than silence. The next analysis shows whether we may attribute this to a plant’s happiness or sadness when being exposed to yodeling. <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"></p>

<p>(2) Codario and Different Emotions</p>

<p>Surprisingly enough, the following analysis indicates that the codario is actually lifting its leaves four times higher and also longer when hearing happy emotions compared to sad ones. For this experiment, we use an <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5955500/">artistic data set</a> with professionals that repeat the same sentences in both very positive and negative ways (you don’t even want to know how often we had to listen to the same sentence <img class="emoji" title=":laughing:" alt=":laughing:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png" height="20" width="20">).</p>

<p><img src="https://i.imgur.com/EkNwJIA.png" alt="Emotions">
<em>The colors are different leaves of the same plant, and one may realize that the left y-axis is different for the different plants.</em></p>

<h2 id="data-collection">Data Collection</h2>

<p>It was quite surprising to learn that plants move differently when a person speaks happily or sadly. Hence, we replicate these early results  with increasing amounts of data and automatize the data processing with Machine Learning techniques. Therefore, we created more of the world’s most important currency of modern times: data.</p>

<h3 id="sound-file-preparations">Sound File Preparations</h3>

<p>With coding a <img class="emoji" title=":snake:" alt=":snake:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png" height="20" width="20"> script, we were able to create different sound files usually constructed in a way that an auditive stimulus is followed by 10s or 60s of silence until the next comes. The emotions (positive vs. negative) would be altered to keep contingent variables like sunlight exposure, time of the day, or room temperature, constant.</p>

<p>Exemplary part snippet of the <a href="https://github.com/plantions/creatingEmotionAudios">sound file creating algorithm</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while counter &lt; (len(happy_sounds) + len(sad_sounds))/2:
    wavset.append(happy_sounds[counter])
    wavset.append("extra_sounds/silence_60s.wav")
    wavset.append(sad_sounds[counter])
    wavset.append("extra_sounds/silence_60s.wav")
    counter += 1
wavs = [AudioSegment.from_wav(wav) for wav in wavset]
combined = wavs[0]
</code></pre></div></div>

<p>During creating these audio files, I learned:</p>
<ul>
  <li>The interplay of sampling rates, a computer’s definition of silence (it is a lot of data), and the complexity of huge data set folder structures.</li>
  <li>How sound waves are being received by human: Sound the way a computer uses them and what we hear is quite different. Hence, we use something called <a href="https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d">MFCCs</a> to simulate different human organs (mainly the interplay of throat, jaw, and tongue) for analysis.</li>
  <li>We also had to adapt the sound files to the recommendation of our mentor, and thus, learned that our flexible sound file creation approach is much better than a manual one.</li>
</ul>

<h3 id="video-screening">Video Screening</h3>

<p>Our mentor provided us with data sets by filming his plants while they were exposed to our different hour-long sound files. We are super grateful for the resulting video files as they contain a lot of visual and audio noise. This is particularly useful because you want realistic data sets, and the world - as we all know - is messy and full of noises. Hence, we have to do a significant amount of data wrangling.</p>

<h2 id="my-quest-automatizing-happiness-analysis-with-machine-learning">My Quest: Automatizing ‘Happiness Analysis with Machine Learning’</h2>

<p>We evaluated different Machine Learning tools (dlib, OpenCV, sci-kit image), and the whole process of moving image <em>aka video</em> analysis to get indirect emotions out of the plants.</p>

<h3 id="a-cloud-supercomputer-machine-learning-tools-and-modern-coding-techniques">A Cloud Supercomputer, Machine Learning Tools, and Modern Coding Techniques</h3>

<p>Next, we set up a cloud-based infrastructure to <strong>not</strong> torture our little MacBooks. This cloud service is named Google Colab and ensures we have a lot of processing power (e.g., 26 GBs of RAM) to process our videos fast. This is how the development infrastructure looks like:</p>

<p><img src="https://i.imgur.com/mWm041e.png" alt="Google Colab Overview"></p>

<p>The current version of the object tracker is introduced hereafter with which we can track many regions of interest at the same time producing comprehensive amounts of data. Please find an example hereafter:</p>

<p><img src="https://i.imgur.com/fe5HRlD.png" alt="Loads of Tracking Points"></p>

<h3 id="overview-of-current-object-tracker">Overview of Current Object Tracker</h3>

<p>The <a href="https://github.com/plantions/video-edge-extractor/blob/master/Emotion_Tracker.ipynb">current object tracker tool</a> consists of five data processing steps. These are uploading a video, choosing hyperparameters for different videos (still kind of an iterative approach), processing and showing the video, the actual data analysis respectively manipulation, and as an output an interactive visualization of the movement. In the following, a brief overview of what is happening in each step, and what I learned when developing it is presented. Please find a video hereafter, of an early version of the object tracker deployed on the mimosa to get an idea about what is happening:</p>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/xgAhZQMkE7U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h4 id="0-step-upload-video">0. Step: Upload Video</h4>

<p><img src="https://i.imgur.com/6VB4WCP.png" alt="Upload Feature"></p>

<p>What is going on?</p>

<ul>
  <li>A simple upload feature to upload any video to the Google Colab environment.</li>
  <li>In the background, all necessary tools (‘<img class="emoji" title=":snake:" alt=":snake:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png" height="20" width="20"> libraries’) are being preloaded.</li>
  <li>Some constants are assigned.</li>
</ul>

<p>What did I learn?</p>

<ul>
  <li>Importing comprehensive video files (through mounting Google Drive for big files).</li>
  <li>Uploading short video files (through interactive upload features).</li>
  <li>To clean up old files that are created during video processing to save space.</li>
</ul>

<h4 id="1-step-choose-hyperparameters">1. Step: Choose Hyperparameters</h4>

<p><img src="https://i.imgur.com/3dJPV0O.png" alt="Hyperparams"></p>

<p>What is going on?</p>

<ul>
  <li>The interactive front-end form takes your inputs of different parameters that are needed for different videos settings.</li>
  <li>The classes for object tracking are initialized. They are constructed to track a region of interest (x, y coordinates).</li>
  <li>The updating class for any additional frame is initialized (using on the x, y - coordinates).</li>
</ul>

<p>What did I learn?</p>

<ul>
  <li>How <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html">Dilation and Erosion</a> work.</li>
  <li>What <a href="https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html">Mask Thresholding</a> does.</li>
  <li>The <a href="https://docs.opencv.org/trunk/dd/d49/tutorial_py_contour_features.html">Contour Feature</a> and its usage.</li>
</ul>

<h4 id="2-step-process--regard-the-video">2. Step: Process &amp; Regard the Video</h4>

<p><img src="https://i.imgur.com/VhWumNT.png" alt="Video Processing"></p>

<p>What is going on?</p>

<ul>
  <li>Every image (30 frames per second, 720 for 12 mintues) is grabbed and analyzed with the parameters set in step 1.</li>
  <li>The data export (x, y coordinates, width and height of the enclosing box, and the ID) are extracted and saved during the processing of the video.</li>
  <li>The resulting video is converted so that it can be viewed right in the Jupyter notebook.</li>
</ul>

<p>What did I learn?</p>

<ul>
  <li>Video processing with Machine Learning library OpenCV4. I have never used that one before and it was kind of the start of the project. It works very well and is a super powerful tool. I had to work through a book to grasp the fundamentals and necessary advanced concepts.</li>
  <li>Data extraction can be very difficult and confusing. Fundamentals of coding language Python were needed. Thank you Paul Steppacher for helping me.</li>
  <li>Different video codecs like MPEG, MP4, etc. are not the same, nor do they work alike. Conversions are crucial and a very iterative process.</li>
</ul>

<h4 id="3-step-analysis">3. Step: Analysis</h4>

<p><img src="https://i.imgur.com/mJLGmpp.png" alt="Data Analysis"></p>

<p>What is going on?</p>

<ul>
  <li>Processing of the data points that were extracted from the video in step 2.</li>
  <li>Extracting of the audio file and creating features that can be used for audio analysis.</li>
  <li>Interventions to sync the audio and video data points.</li>
</ul>

<p>What did I learn?</p>

<ul>
  <li>Creating and manipulating <img class="emoji" title=":panda_face:" alt=":panda_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png" height="20" width="20"> DataFrames including techniques such as: pivoting, duplicates removal, querying, filtering, inversion, etc.</li>
  <li>Audio file processing: sub-processing in a shell in a Jupyter notebook, extracting sample rates, and MFCC features.</li>
  <li>Merging different <img class="emoji" title=":panda_face:" alt=":panda_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png" height="20" width="20"> Dataframes using primary keys and ‘outer/inner’ join distinctions.</li>
</ul>

<p><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html">Data manipulation and analysis with <img class="emoji" title=":panda_face:" alt=":panda_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f43c.png" height="20" width="20"></a></p>

<h4 id="4-step-interactive-visualization">4. Step: Interactive Visualization</h4>

<p><img src="https://i.imgur.com/3X9jQLS.png" alt="Altair"></p>

<p>What is going on?</p>

<ul>
  <li>Resulting data points are melted into a long data structure to fit the graphical API Altair.</li>
  <li>The limit of 5000 data points is fully utilized to visualize the movement of the object and accompanying sounds.</li>
</ul>

<p>What did I learn?</p>

<ul>
  <li>
<a href="https://altair-viz.github.io/getting_started/overview.html">Plotting with Altair</a>.</li>
  <li>Wide vs. Long data formats and how to get back and forth.</li>
  <li>Why Matplotlib and Jupyter Interact were fighting each other.</li>
</ul>

<h4 id="5-additional-features">5. Additional Features</h4>

<p><img src="https://i.imgur.com/3WNYImh.png" alt="Additional Features"></p>

<p>For developing and testing purposes, I incorporated additional functionality which I will not dive into in a deeper manner as of now. These encompass:</p>
<ul>
  <li>A data export feature: ‘Export to Excel’.</li>
  <li>A feature to visualize correlations: ‘Interactive Scatter Plots’. The interactive scatter plot is still in development.</li>
  <li>A video duration measurement feature for cross-checking purposes: ‘Check for Movie Duration’.</li>
  <li>A feature for retrieving the ‘frames per second’ value of a given movie: ‘Getting FPS of Video’.</li>
</ul>

<h3 id="next-steps">Next Steps</h3>

<p>Now, that the data pipeline is functional and initial data analyses are operating, a tool for concluding happy or sad emotions will be added. Hereafter image depicts promising tracking spots.</p>

<p><img src="https://i.imgur.com/FK4dQqk.png" alt="Proper Tracking Spots"></p>

<p>Thank you for comments and your interest!</p>



<div class="pt-5">
  
</div>

</div>
  </main>
  <footer class="mt-auto py-3 text-center">

  <small class="text-muted mb-2">
    <i class="fas fa-code"></i> with <i class="fas fa-heart"></i>
    by <strong>Seb Duerr</strong>
  </small>
  <br> <a href="https://seduerr91.github.io/datenschutz">Disclaimer</a>
  
  <div class="container-fluid justify-content-center">
  
  </div>

</footer>

  
  <!-- GitHub Buttons -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<!-- jQuery CDN -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<!-- Popper.js CDN -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js"></script>

<!-- Bootstrap JS CDN -->
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!-- wow.js CDN & Activation -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/wow/1.1.2/wow.js"></script>
<script> new WOW().init(); </script>

<!-- Initialize all tooltips -->
<script>
$(function () {
    $('[data-toggle="tooltip"]').tooltip()
})
</script>
</body>

</html>